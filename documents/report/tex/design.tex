\section{Methodology}

\section{System Architecture}

\subsection{Network Simulation}

\subsubsection{Fundamental Structure}

\subsubsection{Environment}

\subsubsection{Application Programming Interfaces (API)}

\subsection{Communications Modules}
The simulator itself should not prescribe a set way of performing communications routing, rather it is up to the user to specify the routing algorithm they wish to use. This needs to be done in a way which makes it easy to specify different routing algorithms for different nodes in a simulation, i.e. there is not one set algorithm for each simulation. This is useful, for example, if one wishes to simulate a network of quadcopters backed up by a series of stationary nodes on the ground. In terms of extensibility, it should also be possible to package, distribute, and import implementations for different communications algorithms.

Starting with the first problem, that is the ability to specify different routing algorithms for different nodes in the same simulation, it is clear that the specification of the routing protocol should apply to individual nodes rather than to simulations. To this end, it was decided to break out the communications functions for each node to a separate communications module. This provides a level of abstraction for messageable programs, as they can then make use of generic send and receive functions at the application level of the OSI model, while the communications module has fine grain control over the networking layer. In more practical terms, each messageable must have a communications module associated with it at instantiation, and it is functions of this object that will be called when the unit wishes to send and receive messages. The main benefit of this approach is that it allows flexibility when assigning communications modules - it is possible to either have every node in the simulation use a different type of communications object, or for each node to use an instance of the same object. This provides a very obvious standard for reusing communications code between simulations.

When considering the second problem mentioned above regarding the packaging, distribution, and importing of routing functions, it was determined that this could be best achieved by combining collections of implementations into libraries and allowing the user to make use of these libraries as appropriate when writing a drone program. This makes it easy to mix and match different algorithms from different sources, and means that in order to use a particular approach, one need only link a program against the appropriate communications library. It also allows different contributors to use identical or overlapping namespaces.

It is necessary at this stage to determine exactly what responsibilities a communications module has, as well as the interface it should expose to its messageable. The most obvious function required of a component designed to communicate is to send information. To this end the communication module must be able to take a message and broadcast it to other nodes in the network. Exactly how this is achieved is dependant on the routing algorithm involved, but it is expected that most implementations will receive a message from the messageable, do some intermediate processing (perhaps to determine the best route to take) and then call the \textit{Environment::broadcast} function to pass the message to the simulated hardware (handled by the simulation environment).

If we are able to send messages via a communications module then it stands to reason that we should also be able to receive messages as well. Thus, there must also be a way for the communications module to transfer messages it has received from the environment to its messageable. This can be achieved by providing a callback function to the environment, which can be invoked when there is a message to deliver. An alternative method which instead provides asynchronous message passing is for each communications module to have a queue of incoming messages for it to process, which it should check at regular intervals. Both of these messages have merit, given that interrupt driven message delivery can become problematic when the network is flooded with messages (no other tasks can be done as the message interrupt is constantly called over routine code), and the asynchronous approach can lead to problems when it is paramount that a message be delivered immediately (such as a command to ground a malfunctioning drone). Given that both of these solution are optimal only some of the time, it was decided that both should be included in the simulator as methods of receiving messages.

The module also needs some way of performing tasks which are not triggered by the arrival of new messages. In order to facilitate these time or state driven processes, communications modules need to have a function which is run independently of the send/receive functions which either loops or is called continuously. It is also clear that one should be able to trigger the pushing out or pulling in of messages from this function, to allow for the use of non-data packets (perhaps, again, to determine the optimum route to the destination). For the purposes of simplicity the simulator calls the processing function once when the simulation is run, leaving the problem of how it is terminated.

So, the must be some condition upon which a communications module terminates, lest the simulation run indefinitely. If the threads for a messageable and its communication object were associated with each other then it would be possible to terminate both when the former exits. This can also be achieved by having the program notify the communications module that execution is complete and that it should terminate. The latter option was chosen due to the fact that it simplified the construction of the simulator, and made it easier to modify the relationship between messageable objects and communications modules in the future, for example to change the relationship from one to one to one to many (so that a base station could have separate communication interfaces to other stationary nodes and to aerial units). We devised a de facto standard for this which was to send a message with no addressing information containing only the payload ``KILL''. With the aim of flexibility, developers are at liberty to devise other methods for future communication module implementations.

Underpinning all of the above design decisions is the idea of a common specification of a message. With the view of defining this, it would be sensible to expect all messages to be serialisable as text so that they can be safely transmitted across a real world network. Beyond that it is useful to be able to label messages so that perfunctory filtering can be carried out on incomming traffic without the need for in-depth inspection. Routing model will invariably require more than this, as it does not even include space for a payload or addressing (which are both quite useful when delivering messages), but it serves as a definition of the bare minimum which can be expected. 

Given the above it must be possible for the base message model to be extended by individual implementations, given that the basic object will remain simple enough that every included feature is required by \textit{all} implementations. It is useful to have an extension of the message class which has a payload, and also one which additionally includes addressing information. These modules serve a dual purpose of being able to test the simulator functions as well as being useful to build upon for more complex communications. It is not required to plan such implementations, as it is expected that they will already have their own specifications (such as AODV in our reference code).
		
\subsection{Physical Routing} 

Physical routing in this context is defined as the methods and algorithms applied by the physical entities in the system in order to manage their movement around the environment.

The design of physical routing concerns itself with a number of key problems:
\begin{itemize}
\item Understanding the environment.
\item Finding routes between locations.
\item Avoiding Obstacles.
\item Avoiding other agents in the environment.
\end{itemize}

In the case of this project, the agents are the drones. The nature of the environment is unknown, although due to the scope of the project and the available hardware, it will be assumed that the environment's area is open. A discussion on this problem and possible extensions will be discussed in a later section.

Due to the autonomous nature of the system, each drone must be responsible for its own routing. It might be thought that a way to greatly simplify the problem would be to give the task of routing the drones to the base station with it keeping track of the location of every drone and its environment. However, there are a number of problems to this approach.

Firstly, if all the responsibilities for routing were on one agent, then it would introduce a critical weakness into the system by creating a single point of failure. Should the base station terminate unexpectedly or behave erratically or erroneously, then the entire system would be compromised, potentially leading to disastrous results.

Secondly, allowing the base station to perform all the routing algorithms is making the assumption that it will be in contact with every drone at all times. As is covered in the project specification, it may not be the case that the drones are all within communication distance of the base station. This would, of course, mean that any drone that left the area where it could receive messages from the base station, it would be unable to act. To counteract this, fallback algorithms could be run on the drone, but at that point, the drone may as well run the algorithms in the first place.

		\subsubsection{Environment Representation}
		
The first and most fundamental problem associated with routing around a physical environment is being able to represent, understand, and act on the information about that environment.

The environment, its boundaries, and locations in it will be referred to using a normal 3-dimensional Cartesian coordinate system. The exact size of a 'unit' in this coordinate system will be left intentionally undefined. This is so as to leave the scale of the implementation up to the application of the system. A mapping would be provided to convert from whatever real measurements of the environment the drone is moving through, to the x, y and z coordinates of the virtual representation. As an example, one possible scale and mapping would be in the California forest fires problem. For these drones, a GPS could be used with a mapping between the latitude and longitude provided by the GPS and the x and y coordinates of the Cartesian frame.

		\subsubsection{Pathfinding}
\label{sec:design_physicalrouting_pathfinding}
		
As essential part of physical routing is getting from point A to point B. Given the representation of the environment given above, then this provides a number of options as to how the pathfinding could be done. In most cases, little to no complex pathfinding is required, given that the environemnt is open. In this case, it is only the other drones that would need to be avoided, and this will be discussed below in section \ref{sec:design_physicalrouting_collisionavoidance}.

Assuming obstacles, if there are any, are stored as points in the Cartesian space, then the drone must route around them. This could be done marking an area around the obstacle as impassable, or by constructing a Voronoi map of some other variation of pathing map in order to define the areas the drone can travel through. Given the complexity of creating a Voronoi map and updating it in real time as new obstacles might be detected, simply marking areas around obstacles are restricted would appear to be the best solution.

With this, the environment's 'map' can be viewed as a 3-dimension grid. Using this grid, and any grid areas blocked by obstacles, an A* pathfinding algorithm can be written to navigate around the environment. Given the worst case performance of A* is O(|E|) where |E| is the number of edges or connections in the map and |E| for a 3-dimensional Cartesian map is $n \times 26$, the algorithm is relatively efficient.

For most cases, however, 3 dimensions will not be needed. In most environments, if an area is blocked, it will be blocked for the entire height of the environment as obstacles such as trees, buildings and so on. Due to this, the pathfinding problem can be simplifed to only require 2-dimensions of pathfinding, with the drones adjusting their height as required once the destination has been reached. This means the worst case performance of the A* algorithm becomes $n \times 8$ for 8-way connectivity in a 2-dimensional Cartesian grid.

		\subsubsection{Collision Avoidance}	\label{sec:design_physicalrouting_collisionavoidance}
		
The main routing problem that is present in the specification is dealing with other drones moving about the environment. Pathfinding around immobile objects is relatively simple, as explained above in section \ref{design_physicalrouting_pathfinding}. Conversely, ensuring that no collisions occur between the drones requires real-time updates on the locations of every agent in the system.

Each drone must be responsible for its own collision avoidance around it. Given the drones have no way of easily detecting the presence of other drones, then the drones must continuously broadcast their positions so that any nearby drone can react accordingly. To enable this, each drone will broadcast its location to all other drones in set intervals.

The first, and easiest check to make, is to test if the drones are within range of each other that they could feasibly collide. Conveniently, the limited range of the communication means any drone outside that range is automatically not checked for potential collisions. It is worth noting that because of this, a problem can arise if the drones can travel more distance than the range of communication in the time between location broadcasts. This can be solved by ensuring that the location broadcast is suitable small.

Each drone has a maximum speed it can travel, which is referring to the distance it is allowed to move in a certain time which, again, is mapped to real-world distances. If a message is sent every time step $t$, then the maximum distance a drone can move in that time is $t \times s_max$ where $s_max$ is the maximum speed of the drone. Therefore, a potential collision could occur if the drones are within $(t \times s_max) * 2$ distance of each other in the worst case scenario that both drones are flying towards each other at maximum speed. Using this, an initial check can be done that checks if the drone's are within this distance from each other. If they are not, then no more calculations are required to be done. This will greatly speed up the process for collision avoidance.

In the case where two drones have detected that their paths cross, they must either move to avoid each other, or one must stop. Moving to avoid each other at the same height adds a great deal of extra complication, as well as more computation time on drones' likely limited processors. Because of this, one of the drones will fly over the other.

This then presents the problem of deciding which drone should fly higher. This must be done deterministically by both drones, otherwise a collision may occur anyway. In order to solve this, a priority will be given to each drone. The drone with the higher priority will continue on its route, while the other immediately flies higher and over the top. This should mean that each drone will know whether it will be needing to change z-level without the need to communicate and agree on it each time, hence reducing wait times at potential collisions.

\subsection{Physical Deployment}
One thing that was established early on in the design process for the physical deployment was that each node in a deployed network should have its own simulation (Environment) thread running. This was needed so that LJHFSKDHFSDKFHSDFH The Parrot AR 2 extension that was developed as a case study for the physical deployment was designed with the aim of being a reference for future such extensions. To this end it was important that the different steps required to deploy octoDrone to a piece of hardware were clearly defined and delimited. A consultation of the specification suggested that the process was best divided into the following steps:

\begin{itemize}
\item Developing hardware specific definitions of Drone and Environment functions
\item Developing an intermediate layer which translates commands between the simulator extension and actual hardware
\item Ensuring that the Environment correctly starts and stops any extra processes that are required to fulfil the above steps
\end{itemize}

Focussing on the first point above, there are a number of functions in the \textit{Environment} and \textit{Drone} classes whichmust be redefined in order to send commands to hardware instead of other parts of the simulator. In \textit{Environment} these are \textit{Environment} (the constructors), \textit{Broadcast}, and \textit{Run}. In \textit{Drone} these are \textit{Drone}, \textit{Upkeep}, \textit{Kill}, \textit{Move} and \textit{Turn}.

\subsubsection{Environment::Environment}
The constructor for Environment must perform any reqired instantiation tasks, such as starting the unit listening for incomming communications.

\subsubsection{Libraries}
