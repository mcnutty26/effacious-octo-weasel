This chapter of the report will describe in detail the design decisions which were taken when the project software was being developed. Organised by the major components and libries, the hope is that it will be clear which decisions were taken and why, and how the taking of certain decisions impacted the design choices made for others.

\section{Methodology}

\section{System Architecture}

	\subsection{Network Simulation}

		\subsubsection{Fundamental Design}
			The fundamental design of the simulator focusses on the concept of portability, if implemented correctly, programs written for use on the
			simulator should be trivially transferable to actual drones of any type, so long as those drones are configured correctly and have the
			appropriate environment set up. Another focus of the simulator is on simplicity, it should not be prohibitively difficult to create even
			simple programs. Ideally the simulator should handle as much of the work that is not relevant to actual drones as possible, allowing
			users of the system to focus on what the simulator is simulating rather than getting the simulator to work.

		\subsubsection{Implementation approach}
			There exist two possible approaches to the overall design of the system that were considered for this project: serial simulation
			and parallel simulation.

			Serial simulation would define a strict ordering over which each element inside the simulation would run, each simulation element would be run
			sequentially, allowing a single ``frame" of its program to be run. This provides several advantages, e.g: the simulation would
			have an inherent synchronicity; no drone or other element could ever get ``out of sync" with the rest of the program, creating a
			serial simulation would be significantly less work than a parallel one as no thread interactions would have to be considered. However,
			this approach also has several disadvantages. For example, creating programs for the simulator to run would be significantly less
			intuitive from the users point of view, the function to be ran on each element of the simulator would have to be a single ``frame"
			of that elements simulation, which could lead to confusion (or even the simulator hanging if the user does not realise this). Another
			problem is that this simulation method is not particularity realistic, in a physical deployment each drone and other element will be acting
			independently, not in the strict ordering implied by serial simulation.

			Parallel simulation involves running each element of the simulation on its own thread of execution, each element runs its own programming
			independently of all the others, allowing elements to act simultaneously. The disadvantages and advantages of this method are effectively the
			polar opposites of the disadvantages and advantages of the serial method respectively. In other words, parallel allows for more intuitive
			program writing, creating the entire program at once, and the very concept of the simulation being more realistic as the drones are acting
			simultaneously for advantages. Lack of synchronicity (drones can be several entire loop iterations ahead of other drones for instance) and
			thread interactions for disadvantages.

			Ultimately, parallel simulation was chosen as the method of choice, choosing the more difficult but more realistic and useful approach over
			the easy abstraction. This was primarily chosen due to the fact that the simulator is designed to be a testing bed for actual programs being run
			on actual drones. Thus the easier it is to transfer programs over from one system to the other the better, and allowing users to create the entire
			program in a single function rather than that function representing a single frame of execution allows this to happen more easily.

		\subsubsection{Environment}
			The environment should be the majority of the predefined segment of the simulator. The environment should handle all of the possible
			sensor data that is to be used by the simulator to be gathered by the drones in the simulation. The environment can also be seen as the
			actual simulator itself, whereas drone, base station and communications code are the ``programs" that run on the simulator, thus the
			environment should handle any and all tasks that would not need to be handled in physical deployment. This means that the environment
			handles tasks such as moving drones around, passing messages between them and sending the sensor data that is requested when it is
			requested.

		\subsubsection{Application Programming Interfaces (API)}
			The simulator has to be as flexible as possible, allowing for arbitrary drone programs to be run with it. Due to this, the API was designed
			in order to maximise usability. To this end, the simulator is effectively an unfinished program, the user ``finishes" this program with their
			own code, extending classes as necessary in order to run the code that they want the simulator to handle. This also mirrors how the code would
			interface with the actual physical drones as the code written in the extensions of the drone class is the code that will be running on the
			physical drone itself, similarly code for communications modules and base stations will be simply transferable to actual physical implementations
			of these systems.

\subsection{Communications Modules}
In order to remain as generic as possible, the simulator itself should not prescribe a set way of performing communications routing. Rather, it is up to the user to specify the routing algorithm they wish to use. This needs to be done in a way which makes it easy to specify different routing algorithms for different nodes in a simulation, i.e. there is not one set algorithm for each simulation. This is useful, for example, if one wishes to simulate a network of quadcopters backed up by a series of stationary nodes on the ground. In terms of extensibility, it should also be possible to package, distribute, and import implementations for different communications algorithms.

Starting with the first problem, that is, the ability to specify different routing algorithms for different nodes in the same simulation, it is clear that the specification of the routing protocol should apply to individual nodes rather than to simulations. To this end, it was decided to split the communications functions for each node into a separate communications module. This provides a level of abstraction for messageable programs, as they can then make use of generic send and receive functions at the application level of the OSI model, while the communications module has fine-grain control over the networking layer. In more practical terms, each messageable object must have a communications module associated with it at instantiation, and it is functions of this object that will be called when the unit wishes to send and receive messages. The main benefit of this approach is that it allows flexibility when assigning communications modules - it is possible to either have every node in the simulation use a different type of communications object, or for each node to use an instance of the same object. This provides a very obvious standard for reusing communications code between simulations.

When considering the second problem mentioned above regarding the packaging, distribution, and importing of routing functions, it was determined that this could be best achieved by combining collections of implementations into libraries and allowing the user to make use of these libraries as appropriate when writing a drone program. This makes it easy to mix and match different algorithms from different sources, and means that in order to use a particular approach, one need only link a program against the appropriate communications library. It also allows different contributors to use identical or overlapping namespaces.

\subsubsection{Sending and Receiving Messages}
It is necessary at this stage to determine exactly what responsibilities a communications module has, as well as the interface it should expose to its messageable. The most obvious function required of a component designed to communicate is to send information. To this end the communication module must be able to take a message and broadcast it to other nodes in the network. Exactly how this is achieved is dependant on the routing algorithm involved, but it is expected that most implementations will receive a message from the messageable, do some intermediate processing (perhaps to determine the best route to take) and then call the \textit{Environment::broadcast} function to pass the message to the simulated hardware (handled by the simulation environment).

If we are able to send messages via a communications module then it stands to reason that we should also be able to receive messages as well. Thus, there must also be a way for the communications module to transfer messages it has received from the environment to its messageable. This can be achieved by providing a callback function to the environment, which can be invoked when there is a message to deliver. An alternative method which instead provides asynchronous message passing is for each communications module to have a queue of incoming messages for it to process, which it should check at regular intervals. Both of these messages have merit, given that interrupt driven message delivery can become problematic when the network is flooded with messages (no other tasks can be done as the message interrupt is constantly called over routine code), and the asynchronous approach can lead to problems when it is paramount that a message be delivered immediately (such as a command to ground a malfunctioning drone). Given that both of these solution are optimal only some of the time, it was decided that both should be included in the simulator as methods of receiving messages.

\subsubsection{Processing}
The module also needs some way of performing tasks which are not triggered by the arrival of new messages. In order to facilitate these time or state driven processes, communications modules need to have a method which is run independently of the send/receive functions and either loops or is called continuously. It is also clear that one should be able to trigger the pushing out or pulling in of messages from this function, to allow for the use of non-data packets (perhaps, again, to determine the optimum route to the destination). For the purposes of simplicity the simulator calls the processing function once when the simulation is run, leaving the problem of how it is terminated to the communication module implementation.

So, the must be some condition upon which a communications module terminates, lest the simulation run indefinitely. If the threads for a messageable and its communication object were associated with each other then it would be possible to terminate both when the former exits. This can also be achieved by having the program notify the communications module that execution is complete and that it should terminate. The latter option was chosen due to the fact that it simplified the construction of the simulator, and made it easier to modify the relationship between messageable objects and communications modules in the future, for example to change the relationship from one to one to one to many (so that a base station could have separate communication interfaces to other stationary nodes and to aerial units). We devised a de facto standard for this which was to send a message with no addressing information containing only the payload ``KILL''. With the aim of flexibility, developers are at liberty to devise other methods for future communication module implementations.

<<<<<<< HEAD
Underpinning all of the above design decisions is the idea of a common specification of a message. With the view of defining this, it would be sensible to expect all messages to be serialisable as text so that they can be safely transmitted across a real world network. Beyond that it is useful to be able to label messages so that perfunctory filtering can be carried out on incoming traffic without the need for in-depth inspection. Routing model will invariably require more than this, as it does not even include space for a payload or addressing (which are both quite useful when delivering messages), but it serves as a definition of the bare minimum which can be expected. 
=======
\subsubsection{Defining a Message}
Underpinning all of the above design decisions is the idea of a common specification of a message. With the view of defining this, it would be sensible to expect all messages to be serialisable as text so that they can be safely transmitted across a real world network. Beyond that it is useful to be able to label messages so that perfunctory filtering can be carried out on incomming traffic without the need for in-depth inspection. Routing model will invariably require more than this, as it does not even include space for a payload or addressing (which are both quite useful when delivering messages), but it serves as a definition of the bare minimum which can be expected. 
>>>>>>> a4a04d891d1e06ad74a20a8fa9a188bc7d2f83c0

Given the above it must be possible for the base message model to be extended by individual implementations, given that the basic object will remain simple enough that every included feature is required by \textit{all} implementations. It is useful to have an extension of the message class which has a payload, and also one which additionally includes addressing information. These modules serve a dual purpose of being able to test the simulator functions as well as being useful to build upon for more complex communications. It is not required to plan such implementations, as it is expected that they will already have their own specifications (such as AODV in our reference code).
		
\subsection{Physical Routing} 

Physical routing in this context is defined as the methods and algorithms applied by the physical entities in the system in order to manage their movement around the environment.

The design of physical routing concerns itself with a number of key problems:
\begin{itemize}
\item Understanding the environment.
\item Finding routes between locations.
\item Avoiding Obstacles.
\item Avoiding other agents in the environment.
\end{itemize}

In the case of this project, the agents are the drones. The nature of the environment is unknown, although due to the scope of the project and the available hardware, it will be assumed that the environment's area is open. A discussion on this problem and possible extensions will be discussed in a later section.

Due to the autonomous nature of the system, each drone must be responsible for its own routing. It might be thought that a way to greatly simplify the problem would be to give the task of routing the drones to the base station with it keeping track of the location of every drone and its environment. However, there are a number of problems to this approach.

Firstly, if all the responsibilities for routing were on one agent, then it would introduce a critical weakness into the system by creating a single point of failure. Should the base station terminate unexpectedly or behave erratically or erroneously, then the entire system would be compromised, potentially leading to disastrous results.

Secondly, allowing the base station to perform all the routing algorithms is making the assumption that it will be in contact with every drone at all times. As is covered in the project specification, it may not be the case that the drones are all within communication distance of the base station. This would, of course, mean that any drone that left the area where it could receive messages from the base station, it would be unable to act. To counteract this, fallback algorithms could be run on the drone, but at that point, the drone may as well run the algorithms in the first place.

\subsubsection{Environment Representation}
		
The first and most fundamental problem associated with routing around a physical environment is being able to represent, understand, and act on the information about that environment.

The environment, its boundaries, and locations in it will be referred to using a normal 3-dimensional Cartesian coordinate system. The exact size of a 'unit' in this coordinate system will be left intentionally undefined. This is so as to leave the scale of the implementation up to the application of the system. A mapping would be provided to convert from whatever real measurements of the environment the drone is moving through, to the x, y and z coordinates of the virtual representation. As an example, one possible scale and mapping would be in the California forest fires problem. For these drones, a GPS could be used with a mapping between the latitude and longitude provided by the GPS and the x and y coordinates of the Cartesian frame.

\subsubsection{Pathfinding}
\label{sec:design_physicalrouting_pathfinding}
		
As essential part of physical routing is getting from point A to point B. Given the representation of the environment given above, then this provides a number of options as to how the pathfinding could be done. In most cases, little to no complex pathfinding is required, given that the environemnt is open. In this case, it is only the other drones that would need to be avoided, and this will be discussed below in section \ref{sec:design_physicalrouting_collisionavoidance}.

Assuming obstacles, if there are any, are stored as points in the Cartesian space, then the drone must route around them. This could be done marking an area around the obstacle as impassable, or by constructing a Voronoi map of some other variation of pathing map in order to define the areas the drone can travel through. Given the complexity of creating a Voronoi map and updating it in real time as new obstacles might be detected, simply marking areas around obstacles are restricted would appear to be the best solution.

With this, the environment's 'map' can be viewed as a 3-dimension grid. Using this grid, and any grid areas blocked by obstacles, an A* pathfinding algorithm can be written to navigate around the environment. Given the worst case performance of A* is O(|E|) where |E| is the number of edges or connections in the map and |E| for a 3-dimensional Cartesian map is $n \times 26$, the algorithm is relatively efficient.

For most cases, however, 3 dimensions will not be needed. In most environments, if an area is blocked, it will be blocked for the entire height of the environment as obstacles such as trees, buildings and so on. Due to this, the pathfinding problem can be simplifed to only require 2-dimensions of pathfinding, with the drones adjusting their height as required once the destination has been reached. This means the worst case performance of the A* algorithm becomes $n \times 8$ for 8-way connectivity in a 2-dimensional Cartesian grid.

\subsubsection{Collision Avoidance}
\label{sec:design_physicalrouting_collisionavoidance}
		
The main routing problem that is present in the specification is dealing with other drones moving about the environment. Pathfinding around immobile objects is relatively simple, as explained above in section \ref{design_physicalrouting_pathfinding}. Conversely, ensuring that no collisions occur between the drones requires real-time updates on the locations of every agent in the system.

Each drone must be responsible for its own collision avoidance around it. Given the drones have no way of easily detecting the presence of other drones, then the drones must continuously broadcast their positions so that any nearby drone can react accordingly. To enable this, each drone will broadcast its location to all other drones in set intervals.

The first, and easiest check to make, is to test if the drones are within range of each other that they could feasibly collide. Conveniently, the limited range of the communication means any drone outside that range is automatically not checked for potential collisions. It is worth noting that because of this, a problem can arise if the drones can travel more distance than the range of communication in the time between location broadcasts. This can be solved by ensuring that the location broadcast is suitable small.

Each drone has a maximum speed it can travel, which is referring to the distance it is allowed to move in a certain time which, again, is mapped to real-world distances. If a message is sent every time step $t$, then the maximum distance a drone can move in that time is $t \times s_max$ where $s_max$ is the maximum speed of the drone. Therefore, a potential collision could occur if the drones are within $(t \times s_max) * 2$ distance of each other in the worst case scenario that both drones are flying towards each other at maximum speed. Using this, an initial check can be done that checks if the drone's are within this distance from each other. If they are not, then no more calculations are required to be done. This will greatly speed up the process for collision avoidance.

In the case where two drones have detected that their paths cross, they must either move to avoid each other, or one must stop. Moving to avoid each other at the same height adds a great deal of extra complication, as well as more computation time on drones' likely limited processors. Because of this, one of the drones will fly over the other.

This then presents the problem of deciding which drone should fly higher. This must be done deterministically by both drones, otherwise a collision may occur anyway. In order to solve this, a priority will be given to each drone. The drone with the higher priority will continue on its route, while the other immediately flies higher and over the top. This should mean that each drone will know whether it will be needing to change z-level without the need to communicate and agree on it each time, hence reducing wait times at potential collisions.

\subsection{Physical Deployment}
One thing that was established early on in the design process for the physical deployment was that each node in a deployed network should have its own simulation (Environment) thread running. This was needed so that calls to basic simulator functions could be intercepted, allowing for a seamless transition from simulated hardware to real hardware. Not structuring the physical deployment in this way would have made it incompatible with the base simulator, meaning that programs would have to be rewritten in order to use it. While not explicitly forbidden by the specification, it is clear that this situation should be avoided if at all possible - thus the choice was made to run a simulation environment on each node in a deployment. The Parrot AR 2 extension that was also designed as a case study for the physical deployment was designed with the aim of being a reference for future such extensions. To this end it was important that the different steps required to deploy octoDrone to a piece of hardware were clearly defined and delimited. A consultation of the specification suggested that the process was best divided into the following steps:

\begin{itemize}
\item Developing hardware specific definitions of Drone and Environment functions
\item Developing an intermediate layer which translates commands between the simulator extension and actual hardware
\item Ensuring that the Environment correctly starts and stops any extra processes that are required to fulfil the above steps
\end{itemize}

Focussing on the first point above, there are a number of functions in the \textit{Environment} and \textit{Drone} classes which must be redefined in order to send commands to hardware instead of other parts of the simulator. In \textit{Environment} these are \textit{Environment} (the constructors), \textit{Broadcast}, and \textit{Run}. In \textit{Drone} these are \textit{Drone}, \textit{Upkeep}, \textit{Kill}, \textit{Move} and \textit{Turn}.

\subsubsection{Environment::Broadcast}
This must be modified to send messages to other nodes in the deployment instead of other nodes in the deployment (of which there are none). In the parrot example presented in the project this would entail sending the message to some socket connection (or wrapper on top of one) that would result in the message being sent to all of the other nodes which are in range.

\subsubsection{Drone::Turn and Drone::Move}
Turning and moving is necessarily different when considering a physical deployment. In order to actuate movement commands need to be sent wirelessly to the quadcopter. Whether this is done directly or through a piece of middleware will depend upon the specifics of the platform being targeted, but the calls will originate from these functions. These functions also need to calculate how long the movement will take, either by querying GPS information or by using pre-existing knowledge about the drones hardware.

\subsubsection{Environment::Environment}
The constructor for Environment must perform any required instantiation tasks, such as starting the unit listening for incoming communications. In the sample deployment this is starting a thread to listen to the socket that the broadcast function is sending to, as well as starting a thread to communicate with the drone hardware for movement and turning.

\subsubsection{Drone::Upkeep}
On the subject of movement and turning, the upkeep function is normally used to update the position of the drone in the simulated environment and mark it as having stopped moving when it reaches its destination. While the actuators on a node will take care of actually moving, the hardware must still be told when to stop (and to mark it as stopped in software). This requires communication with the thread set up to communicate (described above).

\subsubsection{Drone::Drone and Drone::Kill}
Because there is no way in the simulator to instruct drones to take off or land (stemming from the fact that there is no concept of a drone being airborne or grounded) this must be handled by the drone constructor and kill functions. The Drone method should instruct the physical quadcopter to take off, and given that Kill is usually used to mark a drone as having finished execution for the run (either through collision, to represent a fault, or just because the program has finished running) it is the method that should be responsible for making the drone land.

\subsubsection{Environment::Run}
The final method that must be redefined for a physical deployment is the Run function of the simulator environment. As part of the teardown performed when a simulation finishes, any extra threads that were created during the running of the program must be stopped or waited for.

\subsubsection{Libraries}
